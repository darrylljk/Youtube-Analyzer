{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sunse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sunse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample data for testing\n",
    "data = {\n",
    "    'text': [\n",
    "        \"I love this video! It's so informative and helpful.\",\n",
    "        \"Great tutorial, I learned a lot from it.\",\n",
    "        \"Not a fan of this content, found it boring.\",\n",
    "        \"Amazing quality and very detailed explanation!\",\n",
    "        \"This is terrible, I wouldn't recommend it to anyone.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this video! It's so informative and hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great tutorial, I learned a lot from it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not a fan of this content, found it boring.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazing quality and very detailed explanation!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is terrible, I wouldn't recommend it to a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  I love this video! It's so informative and hel...\n",
       "1           Great tutorial, I learned a lot from it.\n",
       "2        Not a fan of this content, found it boring.\n",
       "3     Amazing quality and very detailed explanation!\n",
       "4  This is terrible, I wouldn't recommend it to a..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['preprocessed_text'] = df['text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "        word for word in word_tokenize(text.lower()) if word.isalnum() and word not in stop_words\n",
    "    )\n",
    ")\n",
    "\n",
    "# Tokenize for LDA\n",
    "tokenized_text = df['preprocessed_text'].apply(str.split).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: 0.083*\"tutorial\" + 0.083*\"great\" + 0.083*\"learned\" + 0.083*\"love\" + 0.083*\"informative\"\n",
      "Topic 2: 0.068*\"content\" + 0.068*\"fan\" + 0.068*\"found\" + 0.068*\"boring\" + 0.068*\"explanation\"\n"
     ]
    }
   ],
   "source": [
    "# Train the LDA model\n",
    "num_topics = 2\n",
    "num_words = 5\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10)\n",
    "\n",
    "# Extract topics\n",
    "topics = lda_model.print_topics(num_words=num_words)\n",
    "for topic_num, topic_keywords in topics:\n",
    "    print(f\"Topic {topic_num + 1}: {topic_keywords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sunse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sunse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sunse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'emoji' has no attribute 'UNICODE_EMOJI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Analyze emotions for each comment\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m comments:\n\u001b[1;32m---> 14\u001b[0m     emotions \u001b[38;5;241m=\u001b[39m \u001b[43mte\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_emotion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmotions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memotions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\text2emotion\\__init__.py:2716\u001b[0m, in \u001b[0;36mget_emotion\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   2713\u001b[0m     \u001b[38;5;66;03m# print(clean_text)\u001b[39;00m\n\u001b[0;32m   2714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean_text\n\u001b[1;32m-> 2716\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mcleaning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m   2717\u001b[0m emotion_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2718\u001b[0m emotions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHappy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAngry\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurprise\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSad\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFear\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\text2emotion\\__init__.py:2700\u001b[0m, in \u001b[0;36mget_emotion.<locals>.cleaning\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m   2698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcleaning\u001b[39m(text):\n\u001b[0;32m   2699\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m-> 2700\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43memojis_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2701\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|www.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m   2702\u001b[0m     text \u001b[38;5;241m=\u001b[39m removing_contradictions(text)\n",
      "File \u001b[1;32mc:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\text2emotion\\__init__.py:2569\u001b[0m, in \u001b[0;36mget_emotion.<locals>.emojis_extractor\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memojis_extractor\u001b[39m(text):\n\u001b[0;32m   2079\u001b[0m     emoj \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmoji\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mðŸ‘ ðŸ¾\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2080\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mðŸ‘ ðŸ¼\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2081\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mðŸ‘ ðŸ½\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2567\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2568\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFear\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m-> 2569\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43memoji\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUNICODE_EMOJI\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m a:\n\u001b[0;32m   2571\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\text2emotion\\__init__.py:2569\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memojis_extractor\u001b[39m(text):\n\u001b[0;32m   2079\u001b[0m     emoj \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmoji\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mðŸ‘ ðŸ¾\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2080\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mðŸ‘ ðŸ¼\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2081\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mðŸ‘ ðŸ½\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2567\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2568\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFear\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m-> 2569\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43memoji\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUNICODE_EMOJI\u001b[49m)\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m a:\n\u001b[0;32m   2571\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'emoji' has no attribute 'UNICODE_EMOJI'"
     ]
    }
   ],
   "source": [
    "import text2emotion as te\n",
    "\n",
    "# Sample comments\n",
    "comments = [\n",
    "    \"I am so happy with the new update!\",\n",
    "    \"This is frustrating and annoying.\",\n",
    "    \"What a beautiful day, it makes me smile!\",\n",
    "    \"I feel so sad about the ending of the movie.\",\n",
    "    \"The surprise party was amazing, I didn't expect it!\"\n",
    "]\n",
    "\n",
    "# Analyze emotions for each comment\n",
    "for comment in comments:\n",
    "    emotions = te.get_emotion(comment)\n",
    "    print(f\"Comment: {comment}\")\n",
    "    print(f\"Emotions: {emotions}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "c:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning:\n",
      "\n",
      "`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sunse\\.cache\\huggingface\\hub\\models--bhadresh-savani--distilbert-base-uncased-emotion. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the emotion classifier pipeline\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m emotion_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbhadresh-savani/distilbert-base-uncased-emotion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Sample comments\u001b[39;00m\n\u001b[0;32m      7\u001b[0m comments \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am so happy with the new update!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is frustrating and annoying.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe surprise party was amazing, I didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt expect it!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:240\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    246\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the emotion classifier pipeline\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "\n",
    "# Sample comments\n",
    "comments = [\n",
    "    \"I am so happy with the new update!\",\n",
    "    \"This is frustrating and annoying.\",\n",
    "    \"What a beautiful day, it makes me smile!\",\n",
    "    \"I feel so sad about the ending of the movie.\",\n",
    "    \"The surprise party was amazing, I didn't expect it!\"\n",
    "]\n",
    "\n",
    "# Analyze emotions for each comment\n",
    "for comment in comments:\n",
    "    result = emotion_classifier(comment)\n",
    "    print(f\"Comment: {comment}\")\n",
    "    print(f\"Emotion: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sunse\\.cache\\huggingface\\hub\\models--bhadresh-savani--distilbert-base-uncased-emotion. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: I am so happy with the new update!\n",
      "Emotion: [{'label': 'joy', 'score': 0.9985764026641846}]\n",
      "\n",
      "Comment: This is frustrating and annoying.\n",
      "Emotion: [{'label': 'anger', 'score': 0.9877606630325317}]\n",
      "\n",
      "Comment: What a beautiful day, it makes me smile!\n",
      "Emotion: [{'label': 'joy', 'score': 0.9932733774185181}]\n",
      "\n",
      "Comment: I feel so sad about the ending of the movie.\n",
      "Emotion: [{'label': 'sadness', 'score': 0.9990057349205017}]\n",
      "\n",
      "Comment: The surprise party was amazing, I didn't expect it!\n",
      "Emotion: [{'label': 'joy', 'score': 0.7444117069244385}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the emotion classifier pipeline\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "\n",
    "# Sample comments\n",
    "comments = [\n",
    "    \"I am so happy with the new update!\",\n",
    "    \"This is frustrating and annoying.\",\n",
    "    \"What a beautiful day, it makes me smile!\",\n",
    "    \"I feel so sad about the ending of the movie.\",\n",
    "    \"The surprise party was amazing, I didn't expect it!\"\n",
    "]\n",
    "\n",
    "# Analyze emotions for each comment\n",
    "for comment in comments:\n",
    "    result = emotion_classifier(comment)\n",
    "    print(f\"Comment: {comment}\")\n",
    "    print(f\"Emotion: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sunse\\.cache\\huggingface\\hub\\models--unitary--toxic-bert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: You're so stupid and annoying!\n",
      "Toxicity: [{'label': 'toxic', 'score': 0.9878692030906677}]\n",
      "\n",
      "Comment: I love this video, amazing content!\n",
      "Toxicity: [{'label': 'toxic', 'score': 0.0005996284307911992}]\n",
      "\n",
      "Comment: Why would anyone even watch this garbage?\n",
      "Toxicity: [{'label': 'toxic', 'score': 0.8684455156326294}]\n",
      "\n",
      "Comment: Thanks for sharing, this was insightful!\n",
      "Toxicity: [{'label': 'toxic', 'score': 0.0005451672477647662}]\n",
      "\n",
      "Comment: This is the worst thing I've ever seen!\n",
      "Toxicity: [{'label': 'toxic', 'score': 0.5744045972824097}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the toxicity detection pipeline\n",
    "toxicity_classifier = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
    "\n",
    "# Sample comments\n",
    "comments = [\n",
    "    \"You're so stupid and annoying!\",\n",
    "    \"I love this video, amazing content!\",\n",
    "    \"Why would anyone even watch this garbage?\",\n",
    "    \"Thanks for sharing, this was insightful!\",\n",
    "    \"This is the worst thing I've ever seen!\"\n",
    "]\n",
    "\n",
    "# Analyze toxicity for each comment\n",
    "for comment in comments:\n",
    "    result = toxicity_classifier(comment)\n",
    "    print(f\"Comment: {comment}\")\n",
    "    print(f\"Toxicity: {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunse\\OneDrive\\Desktop\\youtube_sentiment\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sunse\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Comments:\n",
      "The production quality is amazing, and the content is very informative. I think there could be more examples to clarify the concepts. The background music is a bit distracting, but overall, it's a great video.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization model\n",
    "def load_summarization_model():\n",
    "    \"\"\"\n",
    "    Loads the summarization pipeline from Hugging Face.\n",
    "    \"\"\"\n",
    "    return pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Synthetic data for testing\n",
    "comments = [\n",
    "    \"I absolutely loved this video! The production quality is amazing, and the content is very informative.\",\n",
    "    \"This is one of the best tutorials I have ever seen. Great job!\",\n",
    "    \"I didn't understand a few points. Could you please elaborate on the part about topic modeling?\",\n",
    "    \"The visuals are stunning, but the explanations could have been better.\",\n",
    "    \"This tutorial was very helpful. Thank you so much!\",\n",
    "    \"I think there could be more examples to clarify the concepts.\",\n",
    "    \"Absolutely loved the way you explained the LDA model. It makes things so much clearer.\",\n",
    "    \"The background music is a bit distracting, but overall, it's a great video.\"\n",
    "]\n",
    "\n",
    "# Combine comments into a single text block\n",
    "combined_comments = \" \".join(comments)\n",
    "\n",
    "# Summarize the comments\n",
    "if __name__ == \"__main__\":\n",
    "    summarizer = load_summarization_model()\n",
    "    summary = summarizer(combined_comments, max_length=100, min_length=30, do_sample=False)\n",
    "\n",
    "    # Print the summary\n",
    "    print(\"Summary of Comments:\")\n",
    "    print(summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Top 20 Most Liked Comments:\n",
      "This is the best explanation I have seen on this topic. I learned so much from this. The visuals are stunning, but the pacing could be improved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    \"author\": [\"User1\", \"User2\", \"User3\", \"User4\", \"User5\"],\n",
    "    \"text\": [\n",
    "        \"This video is amazing, very clear explanation!\",\n",
    "        \"I love how detailed and engaging this tutorial is.\",\n",
    "        \"The visuals are stunning, but the pacing could be improved.\",\n",
    "        \"Great video! I learned so much from this.\",\n",
    "        \"This is the best explanation I have seen on this topic.\"\n",
    "    ],\n",
    "    \"likes\": [50, 40, 35, 60, 80]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract top 20 comments (if available)\n",
    "top_comments = df.sort_values(by=\"likes\", ascending=False).head(20)[\"text\"].tolist()\n",
    "\n",
    "# Combine top comments into a single string\n",
    "comments_text = \" \".join(top_comments)\n",
    "\n",
    "# Load summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Summarize the comments\n",
    "summary = summarizer(comments_text, max_length=50, min_length=10, do_sample=False)\n",
    "print(\"Summary of Top 20 Most Liked Comments:\")\n",
    "print(summary[0][\"summary_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
